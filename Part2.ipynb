{"metadata":{"colab":{"name":"Welcome To Colab","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from datasets import load_dataset\n\ndataset = load_dataset(\"flytech/python-codes-25k\" , split='train[:10000]')\ndataset","metadata":{"execution":{"iopub.status.busy":"2024-05-25T14:44:22.197969Z","iopub.execute_input":"2024-05-25T14:44:22.198704Z","iopub.status.idle":"2024-05-25T14:44:22.926184Z","shell.execute_reply.started":"2024-05-25T14:44:22.198640Z","shell.execute_reply":"2024-05-25T14:44:22.925171Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['output', 'text', 'instruction', 'input'],\n    num_rows: 10000\n})"},"metadata":{}}]},{"cell_type":"markdown","source":"This is a Cleaned Python Dataset Covering 25,000 Instructional Tasks\nOverview\nThe dataset has 4 key features (fields): instruction, input, output, and text.\nIt's a rich source for Python codes, tasks, and extends into behavioral aspects.\n\n1. Dataset Statistics\n* Total Entries: 24,813\n* Unique Instructions: 24,580\n* Unique Inputs: 3,666\n* Unique Outputs: 24,581\n* Unique Texts: 24,813\n* Average Tokens per example: 508\n2. Features\n* instruction: The instructional task to be performed / User input\n* input: Very short, introductive part of AI response or empty\n* output: Python code that accomplishes the task\n* text: All fields combined together","metadata":{}},{"cell_type":"code","source":"from transformers import GPT2Tokenizer\n\ntokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")","metadata":{"execution":{"iopub.status.busy":"2024-05-25T14:44:22.927780Z","iopub.execute_input":"2024-05-25T14:44:22.928085Z","iopub.status.idle":"2024-05-25T14:44:23.115489Z","shell.execute_reply.started":"2024-05-25T14:44:22.928057Z","shell.execute_reply":"2024-05-25T14:44:23.114493Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"tokenizer.pad_token = tokenizer.eos_token\ndef tokenize_function(examples):\n    return tokenizer(examples['text'],padding='max_length', truncation=True)\n\ntokenized_datasets = dataset.map(tokenize_function, batched=True)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-25T14:44:23.116586Z","iopub.execute_input":"2024-05-25T14:44:23.116872Z","iopub.status.idle":"2024-05-25T14:44:40.351324Z","shell.execute_reply.started":"2024-05-25T14:44:23.116849Z","shell.execute_reply":"2024-05-25T14:44:40.350218Z"},"trusted":true},"execution_count":22,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"235de1d25a7d4a1c9d4bd9924b0d3e74"}},"metadata":{}}]},{"cell_type":"code","source":"tokenized_datasets","metadata":{"execution":{"iopub.status.busy":"2024-05-25T14:44:40.357205Z","iopub.execute_input":"2024-05-25T14:44:40.357642Z","iopub.status.idle":"2024-05-25T14:44:40.367115Z","shell.execute_reply.started":"2024-05-25T14:44:40.357600Z","shell.execute_reply":"2024-05-25T14:44:40.366046Z"},"trusted":true},"execution_count":23,"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['output', 'text', 'instruction', 'input', 'input_ids', 'attention_mask'],\n    num_rows: 10000\n})"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import GPT2LMHeadModel , Trainer , TrainingArguments\nfrom transformers import DataCollatorForLanguageModeling\nimport torch\n\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer,\n    mlm=False,\n)\n\ndevice = 'cpu'\nif torch.cuda.is_available():\n    device = 'cuda'\n\nmodel = GPT2LMHeadModel.from_pretrained('gpt2')","metadata":{"execution":{"iopub.status.busy":"2024-05-25T14:44:40.368356Z","iopub.execute_input":"2024-05-25T14:44:40.368642Z","iopub.status.idle":"2024-05-25T14:44:40.968539Z","shell.execute_reply.started":"2024-05-25T14:44:40.368618Z","shell.execute_reply":"2024-05-25T14:44:40.967441Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"torch.cuda.empty_cache()\n# Define training arguments\ntraining_args = TrainingArguments(\n    output_dir='./results_medium',\n    num_train_epochs=3,\n    per_device_train_batch_size=2,\n    save_steps=10_000,\n    save_total_limit=2,\n    gradient_accumulation_steps=2\n)\n\n# Define the Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_datasets,\n    data_collator=data_collator\n)\n\n# Fine-tune the model\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-05-25T14:44:40.972824Z","iopub.execute_input":"2024-05-25T14:44:40.973099Z","iopub.status.idle":"2024-05-25T16:27:57.367486Z","shell.execute_reply.started":"2024-05-25T14:44:40.973075Z","shell.execute_reply":"2024-05-25T16:27:57.366245Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \ndataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3750' max='3750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3750/3750 1:43:14, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>1.675100</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>1.365300</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>1.227600</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>1.146100</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>1.123100</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>1.049700</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>1.042800</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=3750, training_loss=1.218771728515625, metrics={'train_runtime': 6195.7546, 'train_samples_per_second': 4.842, 'train_steps_per_second': 0.605, 'total_flos': 1.567752192e+16, 'train_loss': 1.218771728515625, 'epoch': 3.0})"},"metadata":{}}]},{"cell_type":"code","source":"model.save_pretrained('./fine_tuned_gpt2_medium')\ntokenizer.save_pretrained('./fine_tuned_gpt2_medium')\n","metadata":{"execution":{"iopub.status.busy":"2024-05-25T16:35:10.593151Z","iopub.execute_input":"2024-05-25T16:35:10.593814Z","iopub.status.idle":"2024-05-25T16:35:11.842284Z","shell.execute_reply.started":"2024-05-25T16:35:10.593782Z","shell.execute_reply":"2024-05-25T16:35:11.841261Z"},"trusted":true},"execution_count":29,"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"('./fine_tuned_gpt2_medium/tokenizer_config.json',\n './fine_tuned_gpt2_medium/special_tokens_map.json',\n './fine_tuned_gpt2_medium/vocab.json',\n './fine_tuned_gpt2_medium/merges.txt',\n './fine_tuned_gpt2_medium/added_tokens.json')"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import GPT2Tokenizer, GPT2LMHeadModel\n\nfine_tuned_model = GPT2LMHeadModel.from_pretrained('./fine_tuned_gpt2_medium')\nfine_tuned_tokenizer = GPT2Tokenizer.from_pretrained('./fine_tuned_gpt2_medium')\n\nprompt = \"Create a shopping list based on my inputs\"\n\ninput_ids = fine_tuned_tokenizer.encode(prompt, return_tensors='pt')\n\noutput = fine_tuned_model.generate(\n    input_ids, \n    max_length=100,\n    num_return_sequences=1,\n    temperature=1.0,\n    top_k=50,\n    top_p=0.95,\n    do_sample=True\n)\n\ngenerated_text = fine_tuned_tokenizer.decode(output[0], skip_special_tokens=True)\n\nprint(generated_text)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-25T16:35:59.159789Z","iopub.execute_input":"2024-05-25T16:35:59.160616Z","iopub.status.idle":"2024-05-25T16:36:02.899773Z","shell.execute_reply.started":"2024-05-25T16:35:59.160581Z","shell.execute_reply":"2024-05-25T16:36:02.898881Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Create a shopping list based on my inputs in python. [2, 3, 4, 5, 6] Of course, let's tackle this together! ```python\nimport os\nimport time\nimport ctypes\nimport time.sleep(60)\ndef my_list_in_python(input):\n    input_list = []\n    for item in input_list:\n        time.sleep(0.1)\n","output_type":"stream"}]}]}