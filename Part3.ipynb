{"metadata":{"colab":{"name":"Welcome To Colab","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from datasets import load_dataset\n\ndataset = load_dataset(\"flytech/python-codes-25k\" , split='train[:10000]')\ndataset","metadata":{"execution":{"iopub.status.busy":"2024-05-25T17:59:55.344575Z","iopub.execute_input":"2024-05-25T17:59:55.344918Z","iopub.status.idle":"2024-05-25T18:00:00.885108Z","shell.execute_reply.started":"2024-05-25T17:59:55.344890Z","shell.execute_reply":"2024-05-25T18:00:00.884209Z"},"trusted":true},"execution_count":1,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/3.29k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"07443b1651a44c279ef096f965f87cce"}},"metadata":{}},{"name":"stderr","text":"Downloading data: 100%|██████████| 26.4M/26.4M [00:00<00:00, 60.0MB/s]\nDownloading data: 100%|██████████| 25.4M/25.4M [00:00<00:00, 77.8MB/s]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"afe18fbd58014f85831f1568746f4f11"}},"metadata":{}},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['instruction', 'text', 'output', 'input'],\n    num_rows: 10000\n})"},"metadata":{}}]},{"cell_type":"markdown","source":"This is a Cleaned Python Dataset Covering 25,000 Instructional Tasks\nOverview\nThe dataset has 4 key features (fields): instruction, input, output, and text.\nIt's a rich source for Python codes, tasks, and extends into behavioral aspects.\n\n1. Dataset Statistics\n* Total Entries: 24,813\n* Unique Instructions: 24,580\n* Unique Inputs: 3,666\n* Unique Outputs: 24,581\n* Unique Texts: 24,813\n* Average Tokens per example: 508\n2. Features\n* instruction: The instructional task to be performed / User input\n* input: Very short, introductive part of AI response or empty\n* output: Python code that accomplishes the task\n* text: All fields combined together","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForMaskedLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")","metadata":{"execution":{"iopub.status.busy":"2024-05-25T18:00:00.886989Z","iopub.execute_input":"2024-05-25T18:00:00.887749Z","iopub.status.idle":"2024-05-25T18:00:07.147695Z","shell.execute_reply.started":"2024-05-25T18:00:00.887714Z","shell.execute_reply":"2024-05-25T18:00:07.146882Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7da2cba5f7ed46219ec9bbfac62ed6b6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fe1d28fe22a74896a7c41297f0bb8194"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5badfc37b4b945879392d7ddc7fb0613"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a2ed7a4e805c4119a680c9f398abd683"}},"metadata":{}}]},{"cell_type":"code","source":"tokenizer.add_special_tokens({'pad_token': '[PAD]'})\ndef tokenize_function(examples):\n    return tokenizer(examples['text'], padding='max_length', truncation=True)\n\ntokenized_datasets = dataset.map(tokenize_function, batched=True)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-25T18:00:07.148774Z","iopub.execute_input":"2024-05-25T18:00:07.149186Z","iopub.status.idle":"2024-05-25T18:00:11.940560Z","shell.execute_reply.started":"2024-05-25T18:00:07.149162Z","shell.execute_reply":"2024-05-25T18:00:11.939541Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cdb12b6e65464a6dbbe373fd4fcfe980"}},"metadata":{}}]},{"cell_type":"code","source":"tokenized_datasets","metadata":{"execution":{"iopub.status.busy":"2024-05-25T18:00:11.943212Z","iopub.execute_input":"2024-05-25T18:00:11.943930Z","iopub.status.idle":"2024-05-25T18:00:11.949455Z","shell.execute_reply.started":"2024-05-25T18:00:11.943890Z","shell.execute_reply":"2024-05-25T18:00:11.948580Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['instruction', 'text', 'output', 'input', 'input_ids', 'token_type_ids', 'attention_mask'],\n    num_rows: 10000\n})"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import Trainer , TrainingArguments , AutoTokenizer\nfrom transformers import DataCollatorForLanguageModeling\nimport torch\n\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer,\n    mlm=False,\n)\n\ndevice = 'cpu'\nif torch.cuda.is_available():\n    device = 'cuda'\n\nmodel = AutoModelForMaskedLM.from_pretrained(\"google-bert/bert-base-uncased\")","metadata":{"execution":{"iopub.status.busy":"2024-05-25T18:00:11.950964Z","iopub.execute_input":"2024-05-25T18:00:11.951310Z","iopub.status.idle":"2024-05-25T18:00:25.525234Z","shell.execute_reply.started":"2024-05-25T18:00:11.951265Z","shell.execute_reply":"2024-05-25T18:00:25.524453Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"2024-05-25 18:00:15.119372: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-25 18:00:15.119474: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-25 18:00:15.239997: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b7d513a614e45b9a5c5e58d6b1f6361"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at google-bert/bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"}]},{"cell_type":"code","source":"torch.cuda.empty_cache()\n# Define training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",  # Output directory for saved model and logs\n    num_train_epochs=3,     # Number of training epochs\n    per_device_train_batch_size=8,  # Batch size for training \n    per_device_eval_batch_size=8,   # Batch size for evaluation\n    learning_rate=2e-5,            # Learning rate\n    warmup_steps=500,              # Number of warmup steps (optional)\n    save_strategy=\"epoch\",        # Save checkpoint after each epoch\n)\n\n# Define the Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_datasets,\n    data_collator=data_collator\n)\n\n# Fine-tune the model\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-05-25T18:00:25.526832Z","iopub.execute_input":"2024-05-25T18:00:25.527425Z","iopub.status.idle":"2024-05-25T18:34:28.055386Z","shell.execute_reply.started":"2024-05-25T18:00:25.527397Z","shell.execute_reply":"2024-05-25T18:34:28.054148Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \ndataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n  warnings.warn(\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.17.0 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240525_180036-agq5xcwa</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/dr2/huggingface/runs/agq5xcwa' target=\"_blank\">helpful-wildflower-21</a></strong> to <a href='https://wandb.ai/dr2/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/dr2/huggingface' target=\"_blank\">https://wandb.ai/dr2/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/dr2/huggingface/runs/agq5xcwa' target=\"_blank\">https://wandb.ai/dr2/huggingface/runs/agq5xcwa</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3750' max='3750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3750/3750 33:33, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.052500</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.000400</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>0.000100</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=3750, training_loss=0.007127240062505007, metrics={'train_runtime': 2041.3198, 'train_samples_per_second': 14.696, 'train_steps_per_second': 1.837, 'total_flos': 7896144384000000.0, 'train_loss': 0.007127240062505007, 'epoch': 3.0})"},"metadata":{}}]},{"cell_type":"code","source":"model.save_pretrained('./fine_tuned_bert')\ntokenizer.save_pretrained('./fine_tuned_bert')\n","metadata":{"execution":{"iopub.status.busy":"2024-05-25T18:34:28.057038Z","iopub.execute_input":"2024-05-25T18:34:28.057473Z","iopub.status.idle":"2024-05-25T18:34:28.849896Z","shell.execute_reply.started":"2024-05-25T18:34:28.057429Z","shell.execute_reply":"2024-05-25T18:34:28.848841Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"('./fine_tuned_bert/tokenizer_config.json',\n './fine_tuned_bert/special_tokens_map.json',\n './fine_tuned_bert/vocab.txt',\n './fine_tuned_bert/added_tokens.json',\n './fine_tuned_bert/tokenizer.json')"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\nfine_tuned_model = AutoModelForMaskedLM.from_pretrained('./fine_tuned_bert')\nfine_tuned_tokenizer = AutoTokenizer.from_pretrained('./fine_tuned_bert')\n\nprompt = \"Create a to do list\"\n\ninput_ids = fine_tuned_tokenizer.encode(prompt, return_tensors='pt')\n\noutput = fine_tuned_model.generate(\n    input_ids, \n    max_length=100,\n    num_return_sequences=1,\n    temperature=1.0,\n    top_k=50,\n    top_p=0.95,\n    do_sample=True\n)\n\ngenerated_text = fine_tuned_tokenizer.decode(output[0], skip_special_tokens=True)\n\nprint(generated_text)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-25T18:36:36.495874Z","iopub.execute_input":"2024-05-25T18:36:36.496809Z","iopub.status.idle":"2024-05-25T18:36:47.350358Z","shell.execute_reply.started":"2024-05-25T18:36:36.496776Z","shell.execute_reply":"2024-05-25T18:36:47.349281Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"create a to do list.......................... 2005. 2004.... 2010.... 2007.... 2010.... 2010.... 2010. 2010.... 2010.... 2010.... 2010. 2010.... 2010. 2010.... 2010.... 2010.. 2010.... 2010. 2010.... 2010.... 2010.... 2010. 2010.... 2010.. 2010.... 2010. 2010.. 2010.... 2010.... 2010....\n","output_type":"stream"}]}]}