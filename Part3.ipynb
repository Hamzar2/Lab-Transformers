{"metadata":{"colab":{"name":"Welcome To Colab","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30700,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from datasets import load_dataset\n\ndataset_train = load_dataset(\"McAuley-Lab/Amazon-Reviews-2023\" , 'raw_review_Software' , split='full[2195000:2200000]')\ndataset_val = load_dataset(\"McAuley-Lab/Amazon-Reviews-2023\" , 'raw_review_Software' , split='full[599900:600000]')\ndataset_test = load_dataset(\"McAuley-Lab/Amazon-Reviews-2023\" , 'raw_review_Software' , split='full[699900:700000]')\ndataset_test","metadata":{"execution":{"iopub.status.busy":"2024-05-26T12:28:55.000962Z","iopub.execute_input":"2024-05-26T12:28:55.001287Z","iopub.status.idle":"2024-05-26T12:31:36.521424Z","shell.execute_reply.started":"2024-05-26T12:28:55.001260Z","shell.execute_reply":"2024-05-26T12:31:36.520486Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/datasets/load.py:1461: FutureWarning: The repository for McAuley-Lab/Amazon-Reviews-2023 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/McAuley-Lab/Amazon-Reviews-2023\nYou can avoid this message in future by passing the argument `trust_remote_code=True`.\nPassing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/39.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d053a46bb0a7475da36522552b3befcc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/19.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c35a98beca84ae4844bb2ea0fc0ca47"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/1.87G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c0996bd84d0a4342acec13fa743ecb93"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating full split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc906a1ad42841219635f8d84b0d3b46"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/datasets/load.py:1461: FutureWarning: The repository for McAuley-Lab/Amazon-Reviews-2023 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/McAuley-Lab/Amazon-Reviews-2023\nYou can avoid this message in future by passing the argument `trust_remote_code=True`.\nPassing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['rating', 'title', 'text', 'images', 'asin', 'parent_asin', 'user_id', 'timestamp', 'helpful_vote', 'verified_purchase'],\n    num_rows: 100\n})"},"metadata":{}}]},{"cell_type":"code","source":"dataset_test[50]","metadata":{"execution":{"iopub.status.busy":"2024-05-26T12:31:36.522973Z","iopub.execute_input":"2024-05-26T12:31:36.523250Z","iopub.status.idle":"2024-05-26T12:31:36.534255Z","shell.execute_reply.started":"2024-05-26T12:31:36.523226Z","shell.execute_reply":"2024-05-26T12:31:36.533256Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"{'rating': 5.0,\n 'title': 'Great',\n 'text': 'Great',\n 'images': [],\n 'asin': 'B06Y66GB9T',\n 'parent_asin': 'B06Y66GB9T',\n 'user_id': 'AHAOZODJISG3VGEPREVRDVUAGMPA',\n 'timestamp': 1559611858977,\n 'helpful_vote': 12,\n 'verified_purchase': True}"},"metadata":{}}]},{"cell_type":"markdown","source":"This is a Cleaned Python Dataset Covering 25,000 Instructional Tasks\nOverview\nThe dataset has 4 key features (fields): instruction, input, output, and text.\nIt's a rich source for Python codes, tasks, and extends into behavioral aspects.\n\n1. Dataset Statistics\n* Total Entries: 24,813\n* Unique Instructions: 24,580\n* Unique Inputs: 3,666\n* Unique Outputs: 24,581\n* Unique Texts: 24,813\n* Average Tokens per example: 508\n2. Features\n* instruction: The instructional task to be performed / User input\n* input: Very short, introductive part of AI response or empty\n* output: Python code that accomplishes the task\n* text: All fields combined together","metadata":{}},{"cell_type":"code","source":"import torch\ndevice = 'cpu'\nif torch.cuda.is_available():\n    device = 'cuda'","metadata":{"execution":{"iopub.status.busy":"2024-05-26T12:31:36.535599Z","iopub.execute_input":"2024-05-26T12:31:36.536021Z","iopub.status.idle":"2024-05-26T12:31:40.256689Z","shell.execute_reply.started":"2024-05-26T12:31:36.535989Z","shell.execute_reply":"2024-05-26T12:31:40.255875Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n\ntokenizer.add_special_tokens({'pad_token': '[PAD]'})\ndef tokenize_function(examples):\n    return tokenizer(examples['text'], padding='max_length', truncation=True)\n\ntokenized_datasets_train = dataset_train.map(tokenize_function, batched=True)\ntokenized_datasets_val = dataset_val.map(tokenize_function, batched=True)\ntokenized_datasets_test = dataset_test.map(tokenize_function, batched=True)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-26T12:31:40.257790Z","iopub.execute_input":"2024-05-26T12:31:40.258520Z","iopub.status.idle":"2024-05-26T12:31:45.232421Z","shell.execute_reply.started":"2024-05-26T12:31:40.258485Z","shell.execute_reply":"2024-05-26T12:31:45.231392Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3025ed77594844cbbce576b7f6c40259"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf6a39d303e14061bcc72cf7c6f4e214"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c33ad5fcdc6f41e39fb873a621cd0f97"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"275c4467df484513ab63fe42dbd34d4a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a025e0fe1114418ebac42068009ed008"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/100 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7349e0170bef476a89d33257dfc4b499"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/100 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f5ca65514104f0faac4d274b29d354a"}},"metadata":{}}]},{"cell_type":"code","source":"tokenized_datasets_test","metadata":{"execution":{"iopub.status.busy":"2024-05-26T12:31:45.234748Z","iopub.execute_input":"2024-05-26T12:31:45.235052Z","iopub.status.idle":"2024-05-26T12:31:45.240607Z","shell.execute_reply.started":"2024-05-26T12:31:45.235027Z","shell.execute_reply":"2024-05-26T12:31:45.239661Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['rating', 'title', 'text', 'images', 'asin', 'parent_asin', 'user_id', 'timestamp', 'helpful_vote', 'verified_purchase', 'input_ids', 'token_type_ids', 'attention_mask'],\n    num_rows: 100\n})"},"metadata":{}}]},{"cell_type":"code","source":"!pip install accelerate","metadata":{"execution":{"iopub.status.busy":"2024-05-26T12:31:45.241861Z","iopub.execute_input":"2024-05-26T12:31:45.242391Z","iopub.status.idle":"2024-05-26T12:31:58.599573Z","shell.execute_reply.started":"2024-05-26T12:31:45.242358Z","shell.execute_reply":"2024-05-26T12:31:58.598536Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.29.3)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.1)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.1.2)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.22.2)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.4.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.1.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2024.2.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2.31.0)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.66.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import AutoModelForMaskedLM\nfrom transformers import Trainer , TrainingArguments , AutoTokenizer\nfrom transformers import DataCollatorForLanguageModeling\nfrom accelerate import Accelerator\n\nacc = Accelerator()\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer,\n    mlm=False,\n)\n\nmodel = AutoModelForMaskedLM.from_pretrained(\"google-bert/bert-base-uncased\")\nmodel = acc.prepare(model)","metadata":{"execution":{"iopub.status.busy":"2024-05-26T12:31:58.601034Z","iopub.execute_input":"2024-05-26T12:31:58.601349Z","iopub.status.idle":"2024-05-26T12:32:14.599585Z","shell.execute_reply.started":"2024-05-26T12:31:58.601313Z","shell.execute_reply":"2024-05-26T12:32:14.598781Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"2024-05-26 12:32:02.251247: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-26 12:32:02.251355: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-26 12:32:02.401205: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9db6caf489a243c6acdfeed48cf5e749"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at google-bert/bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"}]},{"cell_type":"code","source":"torch.cuda.empty_cache()\n# Define training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./results_bert\",  # Output directory for saved model and logs\n    num_train_epochs=3,     # Number of training epochs\n    per_device_train_batch_size=4,  # Batch size for training \n    per_device_eval_batch_size=4,   # Batch size for evaluation\n    learning_rate=2e-5,            # Learning rate\n    warmup_steps=500,              # Number of warmup steps (optional)\n    save_strategy=\"epoch\",        # Save checkpoint after each epoch\n    evaluation_strategy=\"epoch\",\n)\n\n# Define the Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_datasets_train,\n    eval_dataset=tokenized_datasets_val,\n    data_collator=data_collator\n)\n\n# Fine-tune the model\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-05-26T12:32:14.600961Z","iopub.execute_input":"2024-05-26T12:32:14.601830Z","iopub.status.idle":"2024-05-26T12:50:14.363148Z","shell.execute_reply.started":"2024-05-26T12:32:14.601799Z","shell.execute_reply":"2024-05-26T12:50:14.362290Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \ndataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n  warnings.warn(\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.17.0 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240526_123219-hadqhn1n</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/dr2/huggingface/runs/hadqhn1n' target=\"_blank\">zesty-armadillo-23</a></strong> to <a href='https://wandb.ai/dr2/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/dr2/huggingface' target=\"_blank\">https://wandb.ai/dr2/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/dr2/huggingface/runs/hadqhn1n' target=\"_blank\">https://wandb.ai/dr2/huggingface/runs/hadqhn1n</a>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1875' max='1875' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1875/1875 17:34, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.275600</td>\n      <td>0.006781</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.001400</td>\n      <td>0.004024</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.000400</td>\n      <td>0.004101</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1875, training_loss=0.07404778439203898, metrics={'train_runtime': 1078.7244, 'train_samples_per_second': 13.905, 'train_steps_per_second': 1.738, 'total_flos': 3948072192000000.0, 'train_loss': 0.07404778439203898, 'epoch': 3.0})"},"metadata":{}}]},{"cell_type":"code","source":"model.save_pretrained('./fine_tuned_bert')\ntokenizer.save_pretrained('./fine_tuned_bert')\n","metadata":{"execution":{"iopub.status.busy":"2024-05-26T12:50:14.364876Z","iopub.execute_input":"2024-05-26T12:50:14.365259Z","iopub.status.idle":"2024-05-26T12:50:15.173381Z","shell.execute_reply.started":"2024-05-26T12:50:14.365225Z","shell.execute_reply":"2024-05-26T12:50:15.172325Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"('./fine_tuned_bert/tokenizer_config.json',\n './fine_tuned_bert/special_tokens_map.json',\n './fine_tuned_bert/vocab.txt',\n './fine_tuned_bert/added_tokens.json',\n './fine_tuned_bert/tokenizer.json')"},"metadata":{}}]},{"cell_type":"code","source":"torch.cuda.empty_cache()\ntest_results = trainer.evaluate(tokenized_datasets_test) \nprint(f\"Test Results: {test_results}\")\n\n# Extract predictions and labels from the test set\ntest_predictions = trainer.predict(tokenized_datasets_test).predictions.argmax(axis=-1)\ntest_labels = tokenized_datasets_test['label']\n\n# Calculate metrics\ntest_accuracy = accuracy_score(test_labels, test_predictions)\ntest_precision = precision_score(test_labels, test_predictions, average='weighted')\n# ... calculate other metrics (recall, f1, confusion matrix) ...\n\nprint(f\"Test Accuracy: {test_accuracy}\")\nprint(f\"Test Precision: {test_precision}\")","metadata":{"execution":{"iopub.status.busy":"2024-05-26T12:53:48.519137Z","iopub.execute_input":"2024-05-26T12:53:48.519521Z"},"trusted":true},"execution_count":null,"outputs":[]}]}