{"metadata":{"colab":{"name":"Welcome To Colab","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from datasets import load_dataset\n\ndataset_train = load_dataset(\"McAuley-Lab/Amazon-Reviews-2023\" , 'raw_review_Software' , split='full[2195000:2200000]')\ndataset_val = load_dataset(\"McAuley-Lab/Amazon-Reviews-2023\" , 'raw_review_Software' , split='full[599900:600000]')\ndataset_test = load_dataset(\"McAuley-Lab/Amazon-Reviews-2023\" , 'raw_review_Software' , split='full[699900:700000]')\ndataset_test","metadata":{"execution":{"iopub.status.busy":"2024-05-26T01:27:14.622924Z","iopub.execute_input":"2024-05-26T01:27:14.623732Z","iopub.status.idle":"2024-05-26T01:27:18.387307Z","shell.execute_reply.started":"2024-05-26T01:27:14.623682Z","shell.execute_reply":"2024-05-26T01:27:18.386175Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/datasets/load.py:1461: FutureWarning: The repository for McAuley-Lab/Amazon-Reviews-2023 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/McAuley-Lab/Amazon-Reviews-2023\nYou can avoid this message in future by passing the argument `trust_remote_code=True`.\nPassing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['rating', 'title', 'text', 'images', 'asin', 'parent_asin', 'user_id', 'timestamp', 'helpful_vote', 'verified_purchase'],\n    num_rows: 100\n})"},"metadata":{}}]},{"cell_type":"code","source":"dataset_test[50]","metadata":{"execution":{"iopub.status.busy":"2024-05-26T01:27:18.388965Z","iopub.execute_input":"2024-05-26T01:27:18.389257Z","iopub.status.idle":"2024-05-26T01:27:18.396803Z","shell.execute_reply.started":"2024-05-26T01:27:18.389229Z","shell.execute_reply":"2024-05-26T01:27:18.395861Z"},"trusted":true},"execution_count":26,"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"{'rating': 5.0,\n 'title': 'Great',\n 'text': 'Great',\n 'images': [],\n 'asin': 'B06Y66GB9T',\n 'parent_asin': 'B06Y66GB9T',\n 'user_id': 'AHAOZODJISG3VGEPREVRDVUAGMPA',\n 'timestamp': 1559611858977,\n 'helpful_vote': 12,\n 'verified_purchase': True}"},"metadata":{}}]},{"cell_type":"markdown","source":"This is a Cleaned Python Dataset Covering 25,000 Instructional Tasks\nOverview\nThe dataset has 4 key features (fields): instruction, input, output, and text.\nIt's a rich source for Python codes, tasks, and extends into behavioral aspects.\n\n1. Dataset Statistics\n* Total Entries: 24,813\n* Unique Instructions: 24,580\n* Unique Inputs: 3,666\n* Unique Outputs: 24,581\n* Unique Texts: 24,813\n* Average Tokens per example: 508\n2. Features\n* instruction: The instructional task to be performed / User input\n* input: Very short, introductive part of AI response or empty\n* output: Python code that accomplishes the task\n* text: All fields combined together","metadata":{}},{"cell_type":"code","source":"import torch\ndevice = 'cpu'\nif torch.cuda.is_available():\n    device = 'cuda'","metadata":{"execution":{"iopub.status.busy":"2024-05-26T01:27:18.398146Z","iopub.execute_input":"2024-05-26T01:27:18.398502Z","iopub.status.idle":"2024-05-26T01:27:18.404232Z","shell.execute_reply.started":"2024-05-26T01:27:18.398447Z","shell.execute_reply":"2024-05-26T01:27:18.403278Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n\ntokenizer.add_special_tokens({'pad_token': '[PAD]'})\ndef tokenize_function(examples):\n    return tokenizer(examples['text'], padding='max_length', truncation=True)\n\ntokenized_datasets_train = dataset_train.map(tokenize_function, batched=True)\ntokenized_datasets_val = dataset_val.map(tokenize_function, batched=True)\ntokenized_datasets_test = dataset_test.map(tokenize_function, batched=True)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-26T01:27:18.406160Z","iopub.execute_input":"2024-05-26T01:27:18.406897Z","iopub.status.idle":"2024-05-26T01:27:20.726063Z","shell.execute_reply.started":"2024-05-26T01:27:18.406863Z","shell.execute_reply":"2024-05-26T01:27:20.724369Z"},"trusted":true},"execution_count":28,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b10b5c5c82d841f8ba68daf7c180e7fe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/100 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e23b71021e924be283f80308bc2cd4ed"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/100 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b6731c6513aa4849a012c690fdd8b941"}},"metadata":{}}]},{"cell_type":"code","source":"tokenized_datasets_test","metadata":{"execution":{"iopub.status.busy":"2024-05-26T01:27:20.728253Z","iopub.execute_input":"2024-05-26T01:27:20.728624Z","iopub.status.idle":"2024-05-26T01:27:20.735083Z","shell.execute_reply.started":"2024-05-26T01:27:20.728598Z","shell.execute_reply":"2024-05-26T01:27:20.734225Z"},"trusted":true},"execution_count":29,"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['rating', 'title', 'text', 'images', 'asin', 'parent_asin', 'user_id', 'timestamp', 'helpful_vote', 'verified_purchase', 'input_ids', 'token_type_ids', 'attention_mask'],\n    num_rows: 100\n})"},"metadata":{}}]},{"cell_type":"code","source":"!pip install accelerate","metadata":{"execution":{"iopub.status.busy":"2024-05-26T01:27:20.736264Z","iopub.execute_input":"2024-05-26T01:27:20.736567Z","iopub.status.idle":"2024-05-26T01:27:33.254977Z","shell.execute_reply.started":"2024-05-26T01:27:20.736542Z","shell.execute_reply":"2024-05-26T01:27:33.253689Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.29.3)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.1)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.1.2)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.22.2)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.4.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.1.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2024.2.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2.31.0)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.66.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import AutoModelForMaskedLM\nfrom transformers import Trainer , TrainingArguments , AutoTokenizer\nfrom transformers import DataCollatorForLanguageModeling\nfrom accelerate import Accelerator\n\nacc = Accelerator()\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer,\n    mlm=False,\n)\n\nmodel = AutoModelForMaskedLM.from_pretrained(\"google-bert/bert-base-uncased\")\nmodel = acc.prepare(model)","metadata":{"execution":{"iopub.status.busy":"2024-05-26T01:27:33.257003Z","iopub.execute_input":"2024-05-26T01:27:33.257412Z","iopub.status.idle":"2024-05-26T01:27:33.974713Z","shell.execute_reply.started":"2024-05-26T01:27:33.257371Z","shell.execute_reply":"2024-05-26T01:27:33.973533Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at google-bert/bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"}]},{"cell_type":"code","source":"torch.cuda.empty_cache()\n# Define training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./results_bert\",  # Output directory for saved model and logs\n    num_train_epochs=3,     # Number of training epochs\n    per_device_train_batch_size=4,  # Batch size for training \n    per_device_eval_batch_size=4,   # Batch size for evaluation\n    learning_rate=2e-5,            # Learning rate\n    warmup_steps=500,              # Number of warmup steps (optional)\n    save_strategy=\"epoch\",        # Save checkpoint after each epoch\n    evaluation_strategy=\"epoch\",\n)\n\n# Define the Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_datasets_train,\n    eval_dataset=tokenized_datasets_val,\n    data_collator=data_collator\n)\n\n# Fine-tune the model\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-05-26T01:27:33.979503Z","iopub.execute_input":"2024-05-26T01:27:33.979935Z","iopub.status.idle":"2024-05-26T01:29:38.344172Z","shell.execute_reply.started":"2024-05-26T01:27:33.979893Z","shell.execute_reply":"2024-05-26T01:29:38.342665Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \ndataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='210' max='1875' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 210/1875 02:02 < 16:19, 1.70 it/s, Epoch 0.33/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[32], line 24\u001b[0m\n\u001b[1;32m     15\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     16\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     17\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     20\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mdata_collator\n\u001b[1;32m     21\u001b[0m )\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Fine-tune the model\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1780\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1778\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1780\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1781\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1782\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1783\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1784\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1785\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2123\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2117\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m   2118\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   2120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2121\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2122\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m-> 2123\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misinf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss_step\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   2124\u001b[0m ):\n\u001b[1;32m   2125\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2126\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[1;32m   2127\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"model.save_pretrained('./fine_tuned_bert')\ntokenizer.save_pretrained('./fine_tuned_bert')\n","metadata":{"execution":{"iopub.status.busy":"2024-05-26T01:29:38.345051Z","iopub.status.idle":"2024-05-26T01:29:38.345403Z","shell.execute_reply.started":"2024-05-26T01:29:38.345224Z","shell.execute_reply":"2024-05-26T01:29:38.345238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_results = trainer.evaluate(tokenized_datasets_test) \nprint(f\"Test Results: {test_results}\")\n\n# Extract predictions and labels from the test set\ntest_predictions = trainer.predict(tokenized_datasets_test).predictions.argmax(axis=-1)\ntest_labels = tokenized_datasets_test['label']\n\n# Calculate metrics\ntest_accuracy = accuracy_score(test_labels, test_predictions)\ntest_precision = precision_score(test_labels, test_predictions, average='weighted')\n# ... calculate other metrics (recall, f1, confusion matrix) ...\n\nprint(f\"Test Accuracy: {test_accuracy}\")\nprint(f\"Test Precision: {test_precision}\")","metadata":{"execution":{"iopub.status.busy":"2024-05-26T01:29:38.347171Z","iopub.status.idle":"2024-05-26T01:29:38.347754Z","shell.execute_reply.started":"2024-05-26T01:29:38.347447Z","shell.execute_reply":"2024-05-26T01:29:38.347484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\nfine_tuned_model = AutoModelForMaskedLM.from_pretrained('./fine_tuned_bert')\nfine_tuned_tokenizer = AutoTokenizer.from_pretrained('./fine_tuned_bert')\n\nprompt = \"Create a to do list\"\n\ninput_ids = fine_tuned_tokenizer.encode(prompt, return_tensors='pt')\n\noutput = fine_tuned_model.generate(\n    input_ids, \n    max_length=100,\n    num_return_sequences=1,\n    temperature=1.0,\n    top_k=50,\n    top_p=0.95,\n    do_sample=True\n)\n\ngenerated_text = fine_tuned_tokenizer.decode(output[0], skip_special_tokens=True)\n\nprint(generated_text)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-26T01:29:38.349404Z","iopub.status.idle":"2024-05-26T01:29:38.349951Z","shell.execute_reply.started":"2024-05-26T01:29:38.349662Z","shell.execute_reply":"2024-05-26T01:29:38.349694Z"},"trusted":true},"execution_count":null,"outputs":[]}]}